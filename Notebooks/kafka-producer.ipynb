{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3316532,"sourceType":"datasetVersion","datasetId":10100}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install confluent_kafka","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\nimport pandas as pd\nfrom confluent_kafka import Producer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip show confluent_kafka","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Setup kafka producer config\n\nconf = {\n    \"bootstrap.servers\":\"pkc-zm3p0.eu-north-1.aws.confluent.cloud:9092\",\n    \"security.protocol\":\"SASL_SSL\",\n    \"sasl.mechanisms\":\"PLAIN\",\n    \"sasl.username\":\"API4GAUSOIXGITLP\",\n    \"sasl.password\":\"cfltuI4w64uzKbCqUtrn1+fQQpcreSdCF3kkFZoxmPuSGNKu/PaAGPQ4YenR8Aow\",\n    \"client.id\":\"json-serial-producer\"\n}\n\nproducer=Producer(conf)\n\ntopic=\"raw_topic\"\n\ndef delivery_report(err,msg):\n    if err:\n        print(f\"Message delivery failed: {err}\")\n    else:\n        print(f\"Message delivery Sucssed: {msg.key()}\")\n\ndef read_checkpoint(checkpoint_file):\n    if os.path.exists(checkpoint_file):\n        with open(checkpoint_file,'r')as file:\n            return int(file.read().strip())\n    return 0\n\ndef write_checkpoint(checkpoint_file,index):\n    with open(checkpoint_file,'w')as file:\n        file.write(str(index))\n        print(f\"checkpoint updated to line: {index}\")\n\n\ndef handle_date(obj):\n    if isinstance(obj, pd.Timestamp):\n        return obj.strftime('%Y-%m-%d %H:%M:%S')\n    raise TypeError(f\"Object of type {type(obj).__name__} is not JSON serializable\")\n    \n#Stream JSON serially\ndef stream_json_serially(file_path,checkpoint_file='/kaggle/working/checkpoint.txt'):\n    last_sent_index = read_checkpoint(checkpoint_file)\n    \n    with open(file_path,'r') as file:\n        for idx,line in enumerate(file):\n            if idx < last_sent_index:\n                continue\n                \n            try:\n                record = json.loads(line)\n                producer.produce(\n                    topic,\n                    key=str(record['review_id']),\n                    value=json.dumps(record,default=handle_date).encode('utf-8'),\n                    callback=delivery_report\n                )\n                \n                producer.flush()\n                \n                write_checkpoint(checkpoint_file, idx + 1)\n                \n            except json.JSONDecodeError as e:\n                print(f\"Failed to decode JSON: {e}\")\n                \nif __name__ == \"__main__\":\n    stream_json_serially('/kaggle/input/yelp-dataset/yelp_academic_dataset_review.json')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}