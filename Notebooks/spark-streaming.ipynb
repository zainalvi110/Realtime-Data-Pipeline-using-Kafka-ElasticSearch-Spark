{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark==3.5.2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip show pyspark","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_json, col, udf\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\nfrom transformers import pipeline\nimport logging\nimport os\n\nlogging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n\ncheckpoint_dir = \"/kaggle/working/checkpoints/kafka_to_mongo\"\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n    \nconfig = {\n    \"kafka\": {\n    \"bootstrap.servers\":\"pkc-zm3p0.eu-north-1.aws.confluent.cloud:9092\",\n    \"security.protocol\":\"SASL_SSL\",\n    \"sasl.mechanisms\":\"PLAIN\",\n    \"sasl.username\":\"API4GAUSOIXGITLP\",\n    \"sasl.password\":\"cfltuI4w64uzKbCqUtrn1+fQQpcreSdCF3kkFZoxmPuSGNKu/PaAGPQ4YenR8Aow\",\n    \"client.id\":\"json-serial-producer\"\n},\n    \"mongodb\": {\n        \"uri\":\"mongodb+srv://spark:Alvi110110@yelpcluster.poj2ewn.mongodb.net/?\",\n        \"database\":\"reviewsdb\",\n        \"collection\":\"enriched_reviews_collection\"\n    }\n}\nsentiment_pipeline = pipeline(\"text-classification\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\ndef analyze_sentiment(text):\n    if text and isinstance(text, str):\n        try:\n            result = sentiment_pipeline(text)[0]\n            return result['label']\n        except Exception as e:\n            logging.error(f\"Error in sentiment analysis: {e}\")\n            return \"Error\"\n    return \"Empty or Invalid\"\n\nsentiment_udf = udf(analyze_sentiment, StringType())\n\ndef read_from_kafka_and_write_to_mongo(spark):\n    topic = \"raw_topic\"\n    \n    schema = StructType([\n        StructField(\"review_id\",StringType()),\n        StructField(\"user_id\",StringType()),\n        StructField(\"business_id\",StringType()),\n        StructField(\"stars\",FloatType()),\n        StructField(\"useful\",IntegerType()),\n        StructField(\"funny\",IntegerType()),\n        StructField(\"cool\",IntegerType()),\n        StructField(\"text\",StringType()),\n        StructField(\"date\",StringType())\n    ])\n    \n    stream_df = (spark.readStream\n                 .format(\"kafka\")\n                 .option(\"kafka.bootstrap.servers\",config['kafka']['bootstrap.servers'])\n                 .option(\"subscribe\",topic)\n                 .option(\"kafka.security.protocol\", config['kafka']['security.protocol'])\n                 .option(\"kafka.sasl.mechanism\",config['kafka']['sasl.mechanisms'])\n                 .option(\"kafka.sasl.jaas.config\",\n                        f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{config[\"kafka\"][\"sasl.username\"]}\" '\n                        f'password=\"{config[\"kafka\"][\"sasl.password\"]}\";')\n                 .option(\"failOnDataLoss\",\"false\")\n                 .load()\n                )\n    parsed_df = stream_df.select(from_json(col('value').cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n    \n    enriched_df = parsed_df.withColumn(\"sentiment\", sentiment_udf(col('text')))\n    \n    query = (enriched_df.writeStream\n             .format(\"mongodb\")\n             .option(\"spark.mongodb.connection.uri\", config['mongodb']['uri'])\n             .option(\"spark.mongodb.database\", config['mongodb']['database'])\n             .option(\"spark.mongodb.collection\", config['mongodb']['collection'])\n             .option(\"checkpointLocation\", checkpoint_dir)\n             .outputMode(\"append\")\n             .start()\n             .awaitTermination()\n            )\n    \nif __name__ == \"__main__\":\n    spark = (SparkSession.builder\n          .appName(\"KafkaStreamToMongo\")\n          .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.2,org.mongodb.spark:mongo-spark-connector_2.12:10.5.0\")\n          .getOrCreate()\n          )\n    read_from_kafka_and_write_to_mongo(spark)\n     ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}